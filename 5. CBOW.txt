5. CBOW
import numpy as np
import re
import tensorflow as tf
import matplotlib.pyplot as plt

# Data Preparation
sentences = '''We are about to study the idea of a computational process.
Computational processes are abstract beings that inhabit computers.
As they evolve, processes manipulate other abstract things called data.
The evolution of a process is directed by a pattern of rules
called a program. People create programs to direct processes. In effect,
we conjure the spirits of the computer with our spells.'''


# Clean Data
# remove special characters
sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)

# remove 1 letter words
sentences = re.sub(r'(?:^| )\w(?:$| )', ' ', sentences).strip()

# lower all characters
sentences = sentences.lower()

# Vocabulary
words = sentences.split()
vocab = set(words)

# Define vocabulary size
vocab_size = len(vocab)
embed_dim = 10#dictates size of vector space
context_size = 2

# Create word-to-index and index-to-word mappings
word_to_ix = {word: i for i, word in enumerate(vocab)}}#enumerate is used to create a numbered list of items from the vocab set.
ix_to_word = {i: word for i, word in enumerate(vocab)}

# Generate training data
data = []
for i in range(2, len(words) - 2):
    context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]
    target = words[i]
    data.append((context, target))
print(data[:5])
# Prepare data for model
context_data = np.array([[word_to_ix[w] for w in context] for context, _ in data])
target_data = np.array([word_to_ix[target] for _, target in data])


# Model Definition
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=2 * context_size),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(vocab_size, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])


# Train the model
history=model.fit(context_data, target_data, epochs=80, batch_size=64)
history.history.keys()
plt.plot(history.history['loss'])
plt.title('Loss per Epoch')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()


# Predict function
def predict(context_words):
    context_idxs = np.array([word_to_ix[word] for word in context_words])
    context_idxs = context_idxs.reshape(1, -1)  # Reshape to match the model's input shape
    pred_idx = np.argmax(model.predict(context_idxs))
    return ix_to_word[pred_idx]


# Test Prediction
context = ['processes', 'manipulate','abstract', 'things']
predicted_word = predict(context)
print(f"Predicted word for context {context}: {predicted_word}")
