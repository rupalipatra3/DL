# -*- coding: utf-8 -*-
"""DL_Ass02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oD1b6XSXwyoZjs2RQrJYrf_U4Ix_40aV
"""

''' Lab Asssignment: 02
Name: Dhruti Kamat
Roll No : 23
BE IT '''

'''Implementing Feedforward neural networks with Keras and TensorFlow
a. Import the necessary packages
b. Load the training and testing data (MNIST/CIFAR10)
c. Define the network architecture using Keras
d. Train the model using SGD
e. Evaluate the network
f. Plot the training loss and accuracy'''

# Commented out IPython magic to ensure Python compatibility.
#importing necessary libraries
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
# %matplotlib inline

#---------------Load the training and testing data(MNIST)-----------------------------
# MNIST-> Modified National Instute of Standard and Technology

#import dataset and split into train and test data
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# to see the length of training datasets
len(x_train)     # len of y_train is also same = 60000

len(x_test)      # len of y_test is also same = 10000

len(y_test)

len(y_train)

x_train.shape

#we want to see first image

x_train[0]

#It is showing image of matrix of size 28*28 pixels(Total 784 features)
#each feature represents the intensity between 0 to 255

#to see how first image look
plt.matshow(x_train[0])

plt.imshow(-x_train[0], cmap="gray")

#normalize the images by scaling pixel intensities to the range 0,1
#Normalization is a technique for organizing data in a database.

x_train = x_train / 255
x_test = x_test / 255

#here 255 is maximum value of intensity that's why it is divided by 255

# Normalizing the RGB codes by dividing it to the max RGB value.
x_train = x_train / 255
x_test = x_test / 255

x_train[0]

#--------------------Creating the model--------------------

model = keras.Sequential([
keras.layers.Flatten(input_shape=(28, 28)),    #Input layer
keras.layers.Dense(128, activation="relu"),    #hidden layer abs
keras.layers.Dense(10, activation="softmax")   #output layer
])
model.summary()

# Sigmoid has less accuracy thats why ReLU is used
# RelU-> Activation function.
#        Rectified Linear Unit
#        Return '0' if input is negative else linear if input is positive.
# softmax-> Activation function
#           chnges input value into values that reach from 0 to 1.

#------------------------ Compile the model--------------------------

model.compile(optimizer="sgd",   # Stochastic Gradient Descent
loss="sparse_categorical_crossentropy",    # crossentropy reduces the loss
metrics=['accuracy'])

history=model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=10)
# epochs means duration/iteration shows loss and accuracy
# data is stored in history

# ---------------------------Evaluate Model-------------------------------

test_loss,test_acc=model.evaluate(x_test,y_test)
print("Loss=%.3f" %test_loss)
print("Accuracy=%.3f" %test_acc)

# ---------------------- Making prediction on New Data----------------------------

n=random.randint(0,9999)
plt.imshow(x_test[n])
plt.show()

#we use predict() on new data
predicted_value=model.predict(x_test)
print("Handwritten number in the image is= %d" %np.argmax(predicted_value[n]))

# ----------------------Plot graph for Accuracy and Loss-----------------------

history.history??

history.history.keys()   #  as data is stored in histoty

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Accuracy and loss on train data
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Training Loss and accuracy')
plt.ylabel('accuracy/Loss')
plt.xlabel('epoch')
plt.legend(['accuracy', 'val_accuracy','loss','val_loss'])
plt.show()